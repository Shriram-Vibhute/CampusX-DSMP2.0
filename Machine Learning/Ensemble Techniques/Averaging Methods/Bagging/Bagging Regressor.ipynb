{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9923af17",
   "metadata": {},
   "source": [
    "### Key Concepts:\n",
    "- **Bagging (Bootstrap Aggregating)**: This method involves training multiple base models (e.g., Decision Trees) on different bootstrap samples of the training data (i.e., sampling with replacement for instances). By default, Bagging does not include feature sampling; it uses all features for each tree unless specified otherwise.\n",
    "- **Random Forest**: This is a specific type of Bagging that adds **node-level feature sampling**. At each node split in a Decision Tree, only a random subset of features is considered for finding the best split. This reduces variance and decorrelates the trees.\n",
    "- **`max_features` in Decision Tree**: The `max_features` hyperparameter in a `DecisionTreeClassifier` (from scikit-learn) controls the number of features to consider when looking for the best split at each node. If set to a value less than the total number of features (e.g., `'sqrt'` or `'log2'`), it enables node-level feature sampling.\n",
    "\n",
    "### How to Achieve Node-Level Sampling in Bagging:\n",
    "- When you use a `BaggingClassifier` with base estimators of `DecisionTreeClassifier`, you can enable node-level feature sampling by setting the `max_features` parameter on the base Decision Tree itself. This means that for each tree in the ensemble, at every node, only a random subset of features is considered for splitting, just like in Random Forest.\n",
    "- Example code:\n",
    "  ```python\n",
    "  from sklearn.ensemble import BaggingClassifier\n",
    "  from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "  # Create a base Decision Tree with node-level feature sampling\n",
    "  base_tree = DecisionTreeClassifier(max_features='sqrt')  # 'sqrt' means sqrt(n_features) per node\n",
    "\n",
    "  # Create a Bagging classifier using this base tree\n",
    "  bagging_clf = BaggingClassifier(\n",
    "      base_estimator=base_tree,\n",
    "      n_estimators=10,\n",
    "      random_state=42\n",
    "  )\n",
    "\n",
    "  # Now, each tree in the bagging ensemble will perform node-level feature sampling\n",
    "  ```\n",
    "- In this case, the BaggingClassifier handles instance sampling (bootstrapping of data points), while the base Decision Tree handles node-level feature sampling via `max_features`. This combination effectively mimics the behavior of a Random Forest.\n",
    "\n",
    "### Important Note:\n",
    "- The `BaggingClassifier` also has its own `max_features` and `bootstrap_features` parameters, but these control **feature sampling at the tree level** (i.e., each tree is trained on a random subset of features for all nodes). This is different from node-level sampling. To achieve node-level sampling, you must set `max_features` on the base Decision Tree, not on the BaggingClassifier.\n",
    "\n",
    "### Why This Works:\n",
    "- By using `max_features` in the base Decision Tree, you introduce randomness at each node split, which decorrelates the trees in the ensemble. This reduces overfitting and improves generalization, similar to Random Forest. However, note that Random Forest may have additional optimizations (e.g., out-of-bag estimation by default), but the core mechanism of node-level sampling is achieved.\n",
    "\n",
    "In summary, with the `max_features` hyperparameter in the base Decision Tree within a BaggingClassifier, you can enable node-level feature sampling, effectively creating a Random Forest-like model using Bagging."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
